# ニューラルネットワークの学習

# 4.1 データから学習する

- ニューラルネットワークはデータから学習できる点
- データから学習するとは、重みパラメータの値をデータから自動で決定できるということ
- すべてのパラメータを手作業によって決めるのは大変
	- ※パラメータ？(学習階数/ユニット数/重みづけ)
- 層を深くしたディープラーニングにもなればほぼ不可能

## 4.1.1 データ駆動

- 機械学習はデータが命
- データからパターンを見つける

### 「5」という数字を見つける

![スクリーンショット 2017-07-04 14.18.04.png](https://qiita-image-store.s3.amazonaws.com/0/7410/8f2904f7-1c42-83f1-f708-e520bc7b6b56.png "スクリーンショット 2017-07-04 14.18.04.png")

- どのようなアルゴリズムが考えられるか？
- 「人」が「特徴量」を抽出する(SIFTや SURF、HOG)
- 画像をベクトルに変換する際に使用した特徴量は、「人」が設計したもの
- 機械学習はこの集められたデータの中から機械が規則性を見つけ出す
- ニューラルネットワークでは人が介在せず、画像をそのまま学習する
- end-to-end machine learning

![スクリーンショット 2017-07-04 10.37.19.png](https://qiita-image-store.s3.amazonaws.com/0/7410/e21748ca-4d27-7e8d-7e39-c93ef16c169c.png "スクリーンショット 2017-07-04 10.37.19.png")

## 4.1.2 訓練データとテストデータ

- 機会学習におけるデータの取扱い
- 訓練データとテストデータ
- 「訓練データ(教師データ)」・・・最適なパラメータを探索
- 「テストデータ」　・・・その訓練したモデルの実力を評価する
- 「汎用能力」・・・まだ見ぬデータ「訓練データにないデータ」に対しての能力。この獲得が目的
- 「過学習」・・・あるデータセットにだけに過度に対応した状態を過学習といいます

## 4.2 損失関数

- 性能の"悪さ"を示す指標
- 教師データに対してどれだけ適応してないか
- 損失関数にマイナスをかけた値はどれだけ性能が悪くないか

## 4.2.1 2 2乗和誤差

- 損失関数に用いられる関数はいくつかある
- よく用いられるのが2乗和誤差

![スクリーンショット 2017-07-04 10.55.11.png](https://qiita-image-store.s3.amazonaws.com/0/7410/1f821c15-48a7-6492-ba36-28822f3f518b.png "スクリーンショット 2017-07-04 10.55.11.png")


```
# coding: utf-8
import numpy as np

def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)

# tは教師データ
# yはニューラルネットワークの出力

# この場合2が正解(one-hot表現)
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
mean_squared_error(np.array(y), np.array(t))
# 0.097500000000000031

# 7が１番確立が高い
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
mean_squared_error(np.array(y), np.array(t))
# 0.59750000000000003

```

## 4.2.2 交差エントロピー誤差

- 交差エントロピー誤差

![スクリーンショット 2017-07-04 14.12.31.png](https://qiita-image-store.s3.amazonaws.com/0/7410/a3cf70b2-1cb7-abfb-414e-da8b69a8aace.png "スクリーンショット 2017-07-04 14.12.31.png")



```
def cross_entropy_error(y, t):
        delta = 1e-7
        return -np.sum(t * np.log(y + delta))

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
cross_entropy_error(np.array(y), np.array(t))
# 0.51082545709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0,  cross_entropy_error(np.array(y), np.array(t))
# 2.3025840929945458
```


## 4.2.3 ミニバッチ学習

- 機械学習は訓練データを使って学習を行います
- 損失関数を求めその値を出来るだけ小さくするようにする
- すべての訓練データの中から無作為に少数のデータを使って学習を行う

- 交差エントロピー誤差の場合の、損失関数の和
- 訓練データが60.000個 ・・計算量が膨大
- 訓練データからある枚数だけ選び出す(ミニバッチ)
- 全体のデータを「近似」 ※視聴率の計測

```
# ※np.random.choice()を使ったランダム抽出
np.random.choice(60000, 10)
```

## 4.2.4 [バッチ対応版]交差エントロピー誤差の実装


## 4.2.5 なぜ損失関数を設定するのか?


-「認識精度」ではなく、「損失関数」を指標にするのか？
- 微分(勾配)を参考にパラメータを調整していく
- 認識精度を指標にすると、なぜパラメータの微分がほとんどの場所で 0 になって しまう
-「認識精度」はパラメータを変化させても影響が出にくい

![スクリーンショット 2017-07-04 13.53.45.png](https://qiita-image-store.s3.amazonaws.com/0/7410/8b8ebae0-76c7-1d35-857a-cbf113d57a15.png "スクリーンショット 2017-07-04 13.53.45.png")
